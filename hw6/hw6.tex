\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

\usepackage{fourier,physics,microtype}

% info for header block in upper right hand corner
\name{Colin Adams}
\class{Math189R SU20}
\assignment{Homework 6}
\duedate{June 2020}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
\newline
\newline
The starter files for problem 2 can be found under the Resource tab on course website. Please print out all the graphs generated by your own code and submit them together with the written part, and make sure you upload the code to your Github repository.\\

\begin{problem}[1]
\textbf{(Murphy 11.2 - EM for Mixtures of Gaussians)} Show that the M step for ML
estimation of a mixture of Gaussians is given by
\begin{align*}
    \mub_k &= \frac{\sum_i r_{ik}\xx_i}{r_k}\\
    \Sigmab_k &= \frac{1}{r_k}\sum_i r_{ik}(\xx_i - \mub_k)(\xx_i - \mub_k)^\T = \frac{1}{r_k}\sum_i r_{ik}\xx_i\xx_i^\T - r_k\mub_k\mub_k^\T.
\end{align*}
\end{problem}
\begin{solution}
From Murphy, the part of $Q$ that is dependent on $\mub_k$ and $\Sigmab_k$ is contained in 
\begin{align*}
l(\mub_k, \Sigmab_k) = \sum_{k,i} r_{ik} \log p(\vb x_i | \thetab_k) = -\frac12 \sum_i r_{ik}\qty[\log(\det (\Sigmab_k) )+ (\vb x_i - \mub_k)^T \Sigmab^{-1}_k (\vb x_i - \mub_k)]
\end{align*}
where we ignore any constants not related $\mub_k$ and $\Sigmab_k$. To solve for $\mub_k$ and $\Sigmab_k$ we will need to take a gradient of $l$. Let's start with $\mub_k$:
\begin{align*}
\dv{l}{\mub_k} = -\frac12 \sum_i r_{ik} \dv{\mub_k}\qty((\vb x_i - \mub_k)^T \Sigmab^{-1}_k (\vb x_i - \mub_k)).
\end{align*}
Note that 
\begin{align*}
(\vb x_i - \mub_k)^T \Sigmab^{-1}_k (\vb x_i - \mub_k) =  \vb x_i^T \Sigmab^{-1}_k \vb x_i - \vb x_i^T \Sigmab^{-1}_k \mub_k - \mub_k^T \Sigmab^{-1}_k \vb x_i + \mub_k^T \Sigmab^{-1}_k \mub_k
\end{align*}
so
\begin{align*}
\dv{\mub_k}\qty((\vb x_i - \mub_k)^T \Sigmab^{-1}_k (\vb x_i - \mub_k)) =  2 \qty(\Sigmab_k^{-1}(\vb x_i - \mub_k))
\end{align*}
which implies
\begin{align*}
\dv{l}{\mub_k} = - \sum_i r_{ik} \Sigmab_k^{-1}(\vb x_i - \mub_k) = - \Sigmab_k^{-1} \sum_i r_{ik} (\vb x_i - \mub_k).
\end{align*}
To find the optimal solution, we need to set this equal to zero, and solve for $\mub_k$ which gives us
\begin{align*}
0 = \sum_i r_{ik} (\vb x_i - \mub_k) \qq{} \Rightarrow \mub_k = \frac{\sum_i r_{ik} \vb x_i}{r_k} \qq{where} r_k \equiv \sum_i r_{ik}.
\end{align*}


Now for the gradient with respect to $\Sigmab_k$, we have
\begin{align*}
\dv{l}{\Sigmab_k} &= -\frac12 \sum_i r_{ik} \dv{\Sigmab_k}\qty(\log(\det (\Sigmab_k) )+  (\vb x_i - \mub_k)^T \Sigmab^{-1}_k (\vb x_i - \mub_k)) \\
 &= -\frac12 \sum_i r_{ik} \qty(\Sigmab^{-T}_k + \dv{\Sigmab_k} (\vb x_i - \mub_k)^T \Sigmab^{-1}_k (\vb x_i - \mub_k)) \\
 &= -\frac12 \sum_i r_{ik} \qty(\Sigmab^{-1}_k -  \Sigmab^{-T}_k (\vb x_i - \mub_k) (\vb x_i - \mub_k)^T \Sigmab^{-T}_k ) \\
 &= -\frac12 \sum_i r_{ik} \qty(\Sigmab^{-1}_k -  \Sigmab^{-1}_k (\vb x_i - \mub_k) (\vb x_i - \mub_k)^T \Sigmab^{-1}_k )
\end{align*}
which we set to zero and solve for $\Sigmab_k$. Doing so, we get
\begin{align*}
0 = \sum_i r_{ik} \qty(\Sigmab^{-1}_k -  \Sigmab^{-1}_k (\vb x_i - \mub_k) (\vb x_i - \mub_k)^T \Sigmab^{-1}_k ) 
\end{align*}
implying the next few steps of algebra:
\begin{align*}
\sum_i r_{ik} \Sigmab^{-1}_k &= \sum_i  \qty(r_{ik} \Sigmab^{-1}_k (\vb x_i - \mub_k) (\vb x_i - \mub_k)^T \Sigmab^{-1}_k ) \\
\Sigmab_k \Sigmab^{-1}_k \sum_i r_{ik}  &= \Sigmab_k\sum_i  \qty(r_{ik} \Sigmab^{-1}_k (\vb x_i - \mub_k) (\vb x_i - \mub_k)^T \Sigmab^{-1}_k ) \\
r_k \vb I   &= \sum_i  \qty(r_{ik} (\vb x_i - \mub_k) (\vb x_i - \mub_k)^T \Sigmab^{-1}_k )\\
r_k \Sigmab_k   &= \sum_i  \qty(r_{ik} (\vb x_i - \mub_k) (\vb x_i - \mub_k)^T \Sigmab^{-1}_k \Sigmab_k)
\end{align*}
and thus, gives us our final answer
\begin{align*}
\Sigmab_k = \frac{1}{r_{k}} \sum_i r_{ik}(\vb x_i - \mub_k) (\vb x_i - \mub_k)^T 
\end{align*}
as desired.







\end{solution}
\newpage



\begin{problem}[2]
\textbf{(SVD Image Compression)}
In this problem, we will use the image of a scary clown online to perform image compression.  In the starter code, we have already load the image into a matrix/array for you. However, you might need internet connection to access the image and therefore successfully run the starter code. The code requires Python library Pillow in order to run.
\newline
\newline 
Plot the progression of the 100 largest singular values for the original image
and a randomly shuffled version of the same image (all on the same plot). In a single figure plot
a grid of four images: the original image, and a rank $k$ truncated SVD approximation of the original
image for $k\in\{2,10,20\}$.

\end{problem}
\begin{solution}
Dropoff and reconstruction plots are below.
\begin{center}
    \includegraphics[width = 4in]{hw6_starter_file/dropoff.png}
    \includegraphics[width = 4in]{hw6_starter_file/reconstruction.png}
\end{center}
Pretty neat to be honest. It's encouraging that the eigenvalues of the dropoff drops off so much for the shuffled data. Makes sense logically but still neat. 
\end{solution}


\end{document}
