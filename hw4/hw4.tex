\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

\usepackage{physics}
\usepackage{fourier}


% info for header block in upper right hand corner
\name{Colin Adams}
\class{Math189R SU20}
\assignment{Homework 4}
\duedate{June 2020}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
\newline
\newline
The starter files can be found under the Resource tab on course website. The graphs for problem 2 generated by the sample solution could be found in the corresponding zipfile. These graphs only serve as references to your implementation. You should generate your own graphs for submission. Please print out all the graphs generated by your own code and submit them together with the written part, and make sure you upload the code to your Github repository.

\begin{problem}[1]
\textbf{(Conditioning a Gaussian)} Note that from Murphy page 113. ``Equation 4.69
is of such importance in this book that we have put a box around it, so you can easily
find it.'' That equation is important. Read through the proof of the result.
Suppose we have a distribution over random variables $\xx = (\xx_1, \xx_2)$ that is
jointly Gaussian with parameters
\[
    \mub = \m{\mub_1\\\mub_2}\;\;\; \Sigmab = \m{\Sigmab_{11}&\Sigmab_{12}\\\Sigmab_{21}&\Sigmab_{22}},
\]
where
\[
    \mub_1 = \m{0\\0}, \;\; \mub_2 = 5, \;\; \Sigmab_{11} = \m{6 & 8\\ 8 & 13}, \;\; \Sigmab_{21}^\T = \Sigmab_{12} = \m{5\\11}, \;\; \Sigmab_{22} = \m{14}.
\]
Compute
\begin{enumerate}[(a)]
    \item The marginal distribution $p(\xx_1)$.
    \item The marginal distribution $p(\xx_2)$.
    \item The conditional distribution $p(\xx_1 | \xx_2)$
    \item The conditional distribution $p(\xx_2 | \xx_1)$
\end{enumerate}

\end{problem}
\begin{solution}
\begin{enumerate}[(a)]
    \item I'm not really sure what they want from me. We have 
    \begin{align*}
        p(\vb x_1) = \mathcal N (\mub_1, \Sigmab_{11}) = \mathcal N \qty( \mqty[0\\0], \mqty[6 & 8\\8&13])
    \end{align*}
    where $\mathcal N $ is the normal distribution. Is that what you want? (Update: glanced at the answer key. Apparently this is what you want.)

    \item Similar to (a) above, we write
    \begin{align*}
        p(\vb x_2) = \mathcal N (\mub_2, \Sigmab_{22}) = \mathcal N (5, 14),
    \end{align*}
    and that's that.

    \item Okay, this one is different at least, so that's exciting. Well, the answer is
    \begin{align*}
        p(\vb x_1 | \vb x_2) = \mathcal N(\mub_{1|2}, \Sigmab_{1|2}) \qq{where} 
        %
        &\mub_{1|2} = \mub_1 + \Sigmab_{12}\Sigmab_{22}^{-1}(\vb x_2 - \mub_2)  = \mqty[0\\0] + \frac{1}{14}\mqty[5\\11](\vb x_2 - 5) \\
        %
        &\Sigmab_{1|2} = \Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1} \Sigmab_{21} = \mqty[6 & 8\\8&13] - \mqty[5\\11] \frac{1}{14} \mqty[5 & 11 ]\\ 
        &\phantom{\Sigmab_{1|2}}=\mqty[6 & 8\\8&13]  - \frac{1}{14}\mqty[25 & 55 \\55 & 121] = \frac1{14}\mqty[ 59 & 57 \\
 57 & 61 ]
    \end{align*}

    \item This one is less exciting because it's the same thing as last time. So, we have
    \begin{align*}
        p(\vb x_2 | \vb x_1) = \mathcal N(\mub_{2|1}, \Sigmab_{2|1}) \qq{where} 
        %
        &\mub_{2|1} = \mub_2 + \Sigmab_{21}\Sigmab_{22}^{-1}(\vb x_1 - \mub_1) = 
    5 + \frac{1}{14}\mqty[5&11]\qty(\vb x_1 - \mqty[0\\0])
         \\
         &\Sigmab_{2|1} = \Sigmab_{22} - \Sigmab_{21} \Sigmab_{11}^{-1} \Sigmab_{12} 
         = 14\qty(1 - \frac{1}{14}\mqty[5&11]\mqty[13 & -8 \\
 -8 & 6] \mqty[5\\11] ) \\
 &\phantom{\Sigmab_{2|1}} = 14\qty( 1 - \frac{171}{14}) = \frac{25}{14}
    \end{align*}
    and we're all done here.
\end{enumerate}
\end{solution}
\newpage

\begin{problem}[2]
(\textbf{MNIST}) 
In this problem, we will use the MNIST dataset, a classic in the deep learning literature as a toy dataset to test
algorithms on, to set up a model for logistic regression and softmax regression. In the starter code, we have already parsed the data for you. However, you might need internet connection to access the data and therefore successfully run the starter code.
\newline
\newline
The problem is this: we have images of handwritten
digits with $28\times 28$ pixels in each image, as well as the label of which digit $0 \leq \texttt{label} \leq 9$ the written
digit corresponds to. Given a new image of a handwritten digit, we want to be
able to predict which digit it is.
The format of the data is \texttt{label, pix-11, pix-12, pix-13, ...}
where \texttt{pix-ij} is the pixel in the \texttt{ith} row and \texttt{jth} column.
\newline
\begin{enumerate}[(a)]
    \item (\textbf{logistic}) Restrict the dataset to only the digits with a label
        of 0 or 1. Implement L2 regularized logistic regression as a model to compute
        $\PP(y=1|\xx)$ for a different value of the regularization parameter $\lambda$.
        Plot the learning curve (objective vs. iteration) when using Newton's Method
        \textit{and} gradient descent.
        Plot the accuracy, precision ($p = \PP(y=1 | \hat y=1)$), recall ($r = \PP(\hat y=1 | y=1)$),
        and F1-score ($F1 = 2pr / (p+r)$) for different values of $\lambda$ (try at least
        10 different values including $\lambda = 0$) on the test set and report the
        value of $\lambda$ which maximizes the accuracy on the test set. What is your
        accuracy on the test set for this model? Your accuracy should definitely be
        over 90\%.

    \item (\textbf{softmax}) Now we will use the whole dataset and predict the label
        of each digit using L2 regularized softmax regression (multinomial logistic
        regression). Implement this using gradient descent, and plot the accuracy
        on the test set for different values of $\lambda$, the regularization parameter.
        Report the test accuracy for the optimal value of $\lambda$ as well as it's
        learning curve. Your accuracy should be over 90\%.

\end{enumerate}
\end{problem}
\begin{solution}
\begin{enumerate}[(a)]
    \item Here is the convergence plot (seems I flipped a negative sign according to the provided plots, but it shows the same information I think):
    \begin{center}
        \includegraphics[width = 3.in]{hw4_starter_files/hw4pr2a_convergence.png}
        \includegraphics[width = 3.in]{hw4_starter_files/hw4pr2a_description_old.png}
    \end{center}
    and if we zoom in on the three to four range for the testing descriptions, we find the optimal regularization parameter is about $\lambda = 3.44$ as shown below:
    \begin{center}
        \includegraphics[width = 3.5in]{hw4_starter_files/hw4pr2a_description.png}
    \end{center}


    \item The accuracy as function of $\lambda$ and the learning curve is plotted below. The optimal parameter was $\lambda = 0.01$ with an accuracy of 92.21\%. 
    \begin{center}
        \includegraphics[width = 3.in]{hw4_starter_files/hw4pr2b_lva.png}
        \includegraphics[width = 3.in]{hw4_starter_files/hw4pr2b_convergence.png}
    \end{center}

\end{enumerate}
\end{solution}
\newpage

\end{document}
