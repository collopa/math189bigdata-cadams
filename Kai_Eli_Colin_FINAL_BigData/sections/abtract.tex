 Human Activity Recognition has recently become a field of focus for machine learning researchers due to the wide-scale proliferation of smartphones and the sensors that they possess. While many researchers have shown success predicting activity based on accelerometer data, their primary focus has on achieving the highest possible accuracy on a single dataset, gathered under controlled conditions. Our goal is to expand on previous work by creating a classifier capable of identifying activities across multiple data sets, with an eye towards an algorithm robust enough to handle real-world data. This approach requires us to take into account the phone's orientation, which has a large impact on accelerometer readings. The phone's orientation is kept consistent within a single academic dataset, so it is usually not taken into account. We attempted to create an orientation invariant representation of accelerometer data by rotating all acceleration axes to a common frame and then aligning them with the principal components of the acceleration over a single feature vector. While we believe this approach is promising, our initial attempt performed worse than simply using the unprocessed data, primarily due to misidentifying sitting and standing. We then created a set of hand-crafted features, some that were based on the raw data and others that were time and coordinate invariant. Our results showed that using the raw data based features yield the best results. These raw data based features give similar performance to when just utilizing the raw data although the dimensionality of our classifier has been reduced more than two orders of magnitude.